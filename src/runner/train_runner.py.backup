"""Training runner - all training logic in src/."""

import torch
from pathlib import Path
from typing import Dict, Any, Optional
import lightning as L
from lightning.pytorch.callbacks import (
    ModelCheckpoint,
    LearningRateMonitor,
    StochasticWeightAveraging,
)

from src.registry import MODEL_REGISTRY, DATASET_REGISTRY, get_model_info, get_dataset_info
from src.experiment import ExperimentTracker, EnhancedTensorBoardLogger
from src.archs.supervised_model import SupervisedModel
from src.archs.diffusion_model import DiffusionModel
from src.utils.config import load_config, merge_config_with_args, save_config


class TrainRunner:
    """
    Centralized training logic.
    All complexity is here, scripts remain simple.
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        Initialize training runner from config.
        
        Args:
            config: Configuration dictionary from YAML + CLI overrides
        """
        # Extract model and dataset names
        model_cfg = config['model']
        data_cfg = config['data']
        
        self.model_name = model_cfg['arch_name']
        self.dataset_name = data_cfg['name']
        
        # Get model and dataset info from registry
        self.model_info = get_model_info(self.model_name)
        self.dataset_info = get_dataset_info(self.dataset_name)
        
        # Store full config
        self.config = config
        
        # Extract common parameters
        self.tag = config.get('tag', None)
        self.resume = config.get('resume', None)
        self.debug = config.get('debug', False)
        
        # Create experiment tracker
        self.tracker = ExperimentTracker()
    
    def _build_config(self, **overrides):
        """Build config with defaults from registry + user overrides."""
        # Calculate batch size based on model type
        if self.model_info.task == 'supervised':
            default_batch_size = self.dataset_info.default_batch_size * 4  # 8 -> 32 (faster supervised)
        else:  # diffusion
            default_batch_size = self.dataset_info.default_batch_size  # 8 (conservative for large diffusion models)
        
        config = {
            # Model defaults
            'arch_name': self.model_name,
            'learning_rate': overrides.get('learning_rate') or self.model_info.default_lr,
            'max_epochs': overrides.get('epochs') or self.model_info.default_epochs,
            
            # Dataset defaults
            'batch_size': overrides.get('batch_size') or default_batch_size,
            'crop_size': overrides.get('crop_size') or self.dataset_info.default_crop_size,
            
            # Hardware
            'gpu': overrides.get('gpu', 0),
            'precision': '32',  # Use fp32 for diffusion models (BCE issue)
            'num_workers': 8,  # More workers for faster data loading
            
            # Training
            'gradient_clip_val': 1.0,
            'accumulate_grad_batches': 1,  # Reduced since batch_size increased
            'check_val_every_n_epoch': 5,  # More frequent validation
            'log_every_n_steps': 20,  # More frequent logging
        }
        
        # Diffusion-specific
        if self.model_info.task == 'diffusion':
            config.update({
                'soft_label_type': overrides.get('soft_label_type', 'none'),
                'soft_label_fg_max': overrides.get('soft_label_fg_max', 11),
                'soft_label_thickness_max': overrides.get('soft_label_thickness_max', 13),
                'soft_label_kernel_ratio': 0.1,
                'soft_label_cache': True,
                'timesteps': overrides.get('timesteps', 1000),
                'use_ema': overrides.get('use_ema', True),
                'ema_decay': overrides.get('ema_decay', 0.9999),
                'num_ensemble': overrides.get('num_ensemble', 1),
            })
        
        # Debug mode adjustments
        if overrides.get('debug'):
            config.update({
                'max_epochs': 2,
                'limit_train_batches': 10,
                'limit_val_batches': 5,
                'check_val_every_n_epoch': 1,
            })
        
        return config
    
    def run(self):
        """Execute training."""
        # Print training info
        self._print_info()
        
        # Create experiment
        experiment = self.tracker.create_experiment(
            model=self.model_name,
            dataset=self.dataset_name,
            config=self.config,
            tag=self.tag,
        )
        
        print(f"\n{'='*60}")
        print(f"Experiment ID: {experiment.id}")
        print(f"Experiment dir: {experiment.dir}")
        print(f"{'='*60}\n")
        
        try:
            # Create datamodule
            datamodule = self._create_datamodule()
            
            # Create model
            model = self._create_model()
            
            # Create callbacks
            callbacks = self._create_callbacks(experiment.dir)
            
            # Create logger
            logger = self._create_logger(experiment.dir)
            
            # Create trainer
            trainer = self._create_trainer(callbacks, logger)
            
            # Train
            print("Starting training...")
            trainer.fit(model, datamodule, ckpt_path=self.resume)
            
            # Get final metrics
            final_metrics = {
                k: v.item() if torch.is_tensor(v) else v
                for k, v in trainer.callback_metrics.items()
                if 'val/' in k
            }
            
            # Find best checkpoint
            best_ckpt = experiment.dir / "checkpoints" / "best.ckpt"
            
            # Finish experiment
            self.tracker.finish_experiment(
                experiment.id,
                final_metrics=final_metrics,
                best_checkpoint=str(best_ckpt) if best_ckpt.exists() else None,
            )
            
            print(f"\n{'='*60}")
            print(f"✅ Training completed!")
            print(f"   Best checkpoint: {best_ckpt}")
            print(f"   Final metrics:")
            for k, v in final_metrics.items():
                print(f"      {k}: {v:.4f}")
            print(f"{'='*60}\n")
            
        except KeyboardInterrupt:
            print("\n⚠️  Training interrupted by user")
            self.tracker.mark_failed(experiment.id, "Interrupted by user")
            raise
        except Exception as e:
            print(f"\n❌ Training failed: {e}")
            self.tracker.mark_failed(experiment.id, str(e))
            raise
    
    def _print_info(self):
        """Print training information."""
        print(f"\n{'='*60}")
        print(f"Training Configuration")
        print(f"{'='*60}")
        print(f"Model: {self.model_name}")
        print(f"  - Description: {self.model_info.description}")
        print(f"  - Parameters: {self.model_info.params:,}")
        print(f"  - Task: {self.model_info.task}")
        print(f"  - Speed: {self.model_info.speed}")
        print(f"\nDataset: {self.dataset_name}")
        print(f"  - Description: {self.dataset_info.description}")
        print(f"  - Samples: {self.dataset_info.num_train} train / "
              f"{self.dataset_info.num_val} val / {self.dataset_info.num_test} test")
        print(f"  - Resolution: {self.dataset_info.resolution}")
        print(f"\nHyperparameters:")
        print(f"  - Learning rate: {self.config['learning_rate']}")
        print(f"  - Batch size: {self.config['batch_size']}")
        print(f"  - Crop size: {self.config['crop_size']}")
        print(f"  - Max epochs: {self.config['max_epochs']}")
        if self.model_info.task == 'diffusion':
            print(f"  - Timesteps: {self.config['timesteps']}")
            print(f"  - Soft label: {self.config['soft_label_type']}")
            print(f"  - Use EMA: {self.config['use_ema']}")
        print(f"{'='*60}\n")
    
    def _create_datamodule(self):
        """Create datamodule from registry."""
        DataModuleClass = self.dataset_info.class_ref
        
        return DataModuleClass(
            train_dir=self.dataset_info.default_train_dir,
            val_dir=self.dataset_info.default_val_dir,
            test_dir=self.dataset_info.default_test_dir,
            crop_size=self.config['crop_size'],
            train_bs=self.config['batch_size'],
        )
    
    def _create_model(self):
        """Create model from registry."""
        common_args = {
            'arch_name': self.model_name,
            'learning_rate': self.config['learning_rate'],
            'experiment_name': f"{self.dataset_name}/{self.model_name}",
            'data_name': self.dataset_name,
        }
        
        if self.model_info.task == 'supervised':
            return SupervisedModel(**common_args)
        else:  # diffusion
            return DiffusionModel(
                **common_args,
                timesteps=self.config['timesteps'],
                soft_label_type=self.config['soft_label_type'],
                soft_label_cache=self.config['soft_label_cache'],
                soft_label_fg_max=self.config['soft_label_fg_max'],
                soft_label_thickness_max=self.config['soft_label_thickness_max'],
                soft_label_kernel_ratio=self.config['soft_label_kernel_ratio'],
                use_ema=self.config['use_ema'],
                ema_decay=self.config['ema_decay'],
                num_ensemble=self.config['num_ensemble'],
            )
    
    def _create_callbacks(self, exp_dir: Path):
        """Create training callbacks."""
        callbacks = [
            # Model checkpoint
            ModelCheckpoint(
                dirpath=exp_dir / "checkpoints",
                filename='best',
                monitor='val/dice',
                mode='max',
                save_last=True,
                verbose=True,
            ),
            # Learning rate monitor
            LearningRateMonitor(logging_interval='epoch'),
        ]
        
        # Add SWA for non-debug mode
        if not self.config.get('debug'):
            callbacks.append(
                StochasticWeightAveraging(swa_lrs=self.config['learning_rate'] / 10)
            )
        
        return callbacks
    
    def _create_logger(self, exp_dir: Path):
        """Create TensorBoard logger."""
        return EnhancedTensorBoardLogger(
            save_dir=exp_dir,
            name="tensorboard",
            version="",
        )
    
    def _create_trainer(self, callbacks, logger):
        """Create Lightning trainer."""
        return L.Trainer(
            max_epochs=self.config['max_epochs'],
            accelerator='gpu',
            devices=[self.config['gpu']],
            precision=self.config['precision'],
            callbacks=callbacks,
            logger=logger,
            log_every_n_steps=self.config['log_every_n_steps'],
            check_val_every_n_epoch=self.config['check_val_every_n_epoch'],
            gradient_clip_val=self.config['gradient_clip_val'],
            accumulate_grad_batches=self.config['accumulate_grad_batches'],
            limit_train_batches=self.config.get('limit_train_batches'),
            limit_val_batches=self.config.get('limit_val_batches'),
        )
